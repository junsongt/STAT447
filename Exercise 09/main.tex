% Template for solutions write-ups, STAT 460/560
% Some basic notation is defined in 'macros/basic-math-macros'

\documentclass{article}
\input{macros/solutions-template}  % DO NOT CHANGE
\input{macros/typesetting-macros}  % DO NOT CHANGE
\input{macros/basic-math-macros} 
\graphicspath{{./figures/}}







\begin{document}



% FILL IN:
%  - YOUR NAME, YOUR EMAIL (self-explanatory)
%  - The assignment number goes in ##
\problemset{Junsong Tang}{junsong.tang@stat.ubc.ca}{Exercise 9}



% WRITE YOUR SOLUTION TO THE FIRST QUESTION
\qsol{A custom MCMC sampler}
\begin{enumerate}
\item 
Denote $\mu = (\mu_1, \mu_2)$ and consider MCMC kernel $K(\mu', c' | \mu, c)$ to be the function of $(\mu, c)$ and outputs $(\mu', c')$. So we could define $K$  to be:
\[K = K_2 \circ K_1\]where $K_1(\mu, c' |\mu, c)$ is a function of $(\mu, c)$ and outputs $(\mu, c')$ and $K_2(\mu', c' | \mu, c')$ is a function of $(\mu, c')$ and outputs $(\mu', c')$. So then the MCMC algorithm can be ilustrated as follows:
\begin{algorithm}
    \caption{custom MCMC}
    \label{algo:mixture}
    \begin{algorithmic}[1]
        \Require $\mu_1, \mu_2, c, y, N$
        \Ensure $\{c_1, \ldots, c_N\}$ and $(\mu_1^{(N)}, \mu_2^{(N)})$

        \For{$i = 1, \ldots, N$}
        \State sample $c' \sim q_c( \cdot | c)$ where $q_c$ could be discrete normal centered at $c$ with hyperparameter sd $\sigma$.
        \State $r \leftarrow \exp(\log\pi(\mu_1, \mu_2, c') - \log\pi(\mu_1, \mu_2, c))$
        \State $c_i \leftarrow \Unif(0,1) < (1 \wedge r)$ ? $c'$ : $c$

        \State sample $(\mu_1', \mu_2') \sim q_{\mu}(\cdot | \mu_1, \mu_2)$
        \State $r \leftarrow \exp(\log\pi(\mu_1', \mu_2', c') - \log\pi(\mu_1, \mu_2, c'))$
        \State $(\mu_1, \mu_2) \leftarrow \Unif(0,1) < (1 \wedge r)$ ? $(\mu_1', \mu_2')$ : $(\mu_1, \mu_2)$
        \EndFor
        \State $(\mu_1^{(N)}, \mu_2^{(N)}) = (\mu_1, \mu_2)$
    \end{algorithmic}
\end{algorithm}

Since each kernel $K_i$ will lead to irreducible and invariant MCMC, hence when we composite the kernels, and consider the kernels as transition probabilities, then such kernel composition is just kernel alternation. Hence by the result from lecture, $K$ has irreducibility and invariance.



\item 
\begin{lstlisting}[language=R]
mcmc = function(means, change_point, y, n_iterations) {
    change_point_trace = rep(-1, n_iterations)
    for (i in 1:n_iterations) {
        # K2 for c
        proposed_c = floor(rnorm(1, change_point, sd) + 0.5) # discrete normal proposal
        ratio = exp(log_joint(means, proposed_c, y) - log_joint(means, change_point, y)) # M-H ratio
        change_point = ifelse(runif(1) < min(ratio,1), proposed_c, change_point) # acceptance
        change_point_trace[i] = change_point
        # K1 for mu
        proposed_mu = rnorm(2, means, 0.1)
        ratio = exp(log_joint(proposed_mu, change_point, y) - log_joint(means, change_point, y))
        if (runif(1) < min(ratio, 1)) {
        means = proposed_mu
        }
        # means = ifelse(runif(1) < min(ratio,1), proposed_mu, means) # !!! VERY ANNOYING BUG
    }
    # Return:
    # - the trace of the change points (for question 1) 
    # - the means at the last iteration (for question 2)
    return(
        list(
        change_point_trace = change_point_trace, 
        last_iteration_means = means 
        )
    )
}
\end{lstlisting}



\end{enumerate}



\qsol{MCMC correctness testing}
\begin{enumerate}
\item 
\begin{lstlisting}[language=R]
forward = function(synthetic_data_size) {
    means = runif(2, 0.1, 0.9)
    change_point = ceiling(runif(1, 0, synthetic_data_size))
    data = numeric(synthetic_data_size)
    for (i in 1:synthetic_data_size) {
        index = ifelse(i >= change_point, 1, 0) + 1
        data[i] = 5 * rbeta(1, means[index]*5, (1-means[index])*5)
    } 
    return(list(
        means = means,
        change_point = change_point,
        data = data
    ))
}
\end{lstlisting}


\item 
\begin{lstlisting}[language=R]
# 2
forward_only = replicate(1000, forward_posterior(5, 0))
with_mcmc = replicate(1000, forward_posterior(5, 200))

ks.test(forward_only, with_mcmc)
# p-value = 0.9135
\end{lstlisting}
\end{enumerate}

\qsol{Using your sampler for data analysis}
\begin{enumerate}
\item 
\begin{lstlisting}[language=R]
sd = 26
initial_mu = runif(2, 0.1, 0.9)
initial_c = ceiling(runif(1, 0, length(food_data)))
samples = mcmc(initial_mu, initial_c, food_data, 10000)
plot(samples$change_point_trace)
plot(samples$change_point_trace[5000:10000], axes = TRUE, type = "o", col = rgb(red = 0, green = 0, blue = 0, alpha = 0.2))  
\end{lstlisting}
\begin{figure}[H]
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{trace_all.png}
        \caption{Trace plot of all}
        \label{fig:trace_all}
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{trace_tail.png}
        \caption{Trace plot of tail}
        \label{fig:trace_tail}
    \end{minipage}
\end{figure}
If we use small standard deviation for the discrete proposal, by the hint from problem one, say less than $5$, then the mixing is not so good as the change point seems to have several convergence points, while if we take the bigger sd, say $26$, then the change point will have a lot fewer convergence points which is shown in Figure \ref{fig:trace_all} and Figure \ref{fig:trace_tail}. This indicates fast mixing. 


\item 
\begin{lstlisting}[language=R]
hist(samples$change_point_trace[5000:10000]) 
\end{lstlisting}
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{hist.png}
\caption{Histogram of tail}
\label{fig:hist}
\end{figure}
From the histogram in Figure \ref{fig:hist}, we can see that the posterior distirbution of change point has most of the mass centered around $30$ with the mode around $28$. So if we use MAP estimator to estimate what time could most likely be the change point, then that corresponds to the note given below saying that the true change point is actually $28$.

\end{enumerate}
 
\end{document}

