% Template for solutions write-ups, STAT 460/560
% Some basic notation is defined in 'macros/basic-math-macros'

\documentclass{article}
\input{macros/solutions-template}  % DO NOT CHANGE
\input{macros/typesetting-macros}  % DO NOT CHANGE
\input{macros/basic-math-macros} 
\graphicspath{{./figures/}}







\begin{document}



% FILL IN:
%  - YOUR NAME, YOUR EMAIL (self-explanatory)
%  - The assignment number goes in ##
\problemset{Junsong Tang}{junsong.tang@stat.ubc.ca}{Exercise 8}



% WRITE YOUR SOLUTION TO THE FIRST QUESTION
\qsol{Calibration analysis via cross-validation}
\begin{enumerate}
\item 
Below is the Stan code for the ``hubble\_pred'' model:
\begin{lstlisting}[language=R]
  data {
    int<lower=0> N; // number of observations
    vector[N] xs;   // independent variable
    vector[N] ys;   // dependent variable
    real x_pred;  //independent variable for the left-out point
  }

  parameters {
    real slope;
    real<lower=0> sigma;
  }

  model {
    // prior
    slope ~ student_t(3, 0, 100);
    sigma ~ exponential(0.001);

    // likelihood
    ys ~ normal(slope*xs, sigma);
  }

  generated quantities {
    real y_pred = normal_rng(slope * x_pred, sigma); 
  }
\end{lstlisting}


\item 
\begin{lstlisting}[language=R]
  set.seed(2025)
  suppressPackageStartupMessages(require(rstan))
  suppressPackageStartupMessages(require(ggplot2))
  suppressPackageStartupMessages(require(dplyr))
  df = read.csv(url("https://github.com/UBC-Stat-ML/web447/raw/0d6eaa346d78abe4cd125e8fc688c9074d6331d9/data/hubble-1.csv")) %>%
  rename(distance = R..Mpc.) %>%
  rename(velocity = v..km.sec.)
  df$velocity = df$velocity/1000

  N_obs = nrow(df)
  N_train = N_obs-1
  train_test_dta = list(
      N  = N_train,
      xs = df$distance[-N_obs], 
      ys = df$velocity[-N_obs], 
      x_pred = df$distance[N_obs]
  )
  fit = sampling(stan_model("hubble_predict.stan"), 
  data=list(N=train_test_dta$N, ys=train_test_dta$ys, xs=train_test_dta$xs, x_pred=train_test_dta$x_pred), 
  iter=5000, control = list(max_treedepth = 15))

  quantile(extract(fit)$y_pred, prob=c(0.1, 0.9))
  # 0.485201900838154, 1.13324592497241
\end{lstlisting}
It can be seen that the leave-one-out $80\%$ credible interval for ``y\_pred'' quantity is: $[0.48, 1.13]$.

\item 
\begin{lstlisting}[language=R]
  # 3
  ci_limits = matrix(, nrow = N_obs, ncol = 2)
  for (i in (1:N_obs)) {
      trained = list(N=N_train, xs=df$distance[-i], ys=df$velocity[-i], x_pred=df$distance[i])
      fit_i = sampling(stan_model("hubble_predict.stan"), 
      data=list(N=trained$N, ys=trained$ys, xs=trained$xs, x_pred=trained$x_pred), 
      iter=5000, control = list(max_treedepth = 15))
      ci_limits[i,] = as.vector(quantile(extract(fit_i)$y_pred, prob=c(0.1, 0.9)))
  }
  merged_df = df %>% 
  bind_cols(data.frame(CI_L = ci_limits[,1], CI_R = ci_limits[,2])) %>% 
  mutate(Inside_CI = (velocity >= CI_L & velocity <= CI_R)) 
  merged_df %>% 
    ggplot(aes(x = 1:N_obs, y = velocity, ymin = CI_L, ymax = CI_R, color=Inside_CI)) +
    geom_point() + 
    geom_errorbar() +
    theme_minimal() +
    labs(x = "Point", y = "Velocity")
\end{lstlisting}
The plot of $80\%$ credible intervals for all observations are given as in Figure \ref{fig:ci}, hence it can be observed that the number of intervals that fail to capture the true points is $4$, so the proportion of such successful intervals is $\frac56 \approx 0.83$, which is similar to the nominal coverage level.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth, height=0.4\textheight]{ci.png}
  \caption{Plot of $80\%$ credible intervals for each observation}
  \label{fig:ci}
\end{figure}

\end{enumerate}



\qsol{Estimating asymptotic variance}
\begin{lstlisting}[language=R]
  # function
  estimate_asymptotic_variance = function(gamma, proposal_sd, C, S) {
    sum_E = 0 # the cumulative sum of all E_s^{(c)} to compute the mean: E
    sum_Ec_sq = 0 # the cumulative sum of all (E_s^{(c)})^2 to compute the 2nd moment
    init_so_far = numeric(C) # the initial pts so far to avoid duplicate initial pt
    # could use equivalent hashmap in R to keep init_so_far to have O(1) find()
    counter = 0
    for (c in (1:C)) {    
        init_pt = rnorm(1, 0, 1)
        while (init_pt %in% init_so_far) {
            init_pt = rnorm(1, 0, 1)
        }
        counter = counter + 1
        init_so_far[counter] = init_pt
        E_c = mean(simple_mh(gamma, init_pt, S, proposal_sd))
        sum_E = sum_E + E_c * S
        sum_Ec_sq = sum_Ec_sq + E_c^2
    }
    return ((S/C)*sum_Ec_sq - S*(sum_E/(C*S))^2)
  }
\end{lstlisting}


\begin{lstlisting}[language=R]
  # generate the data frame for variances
  gamma = function(x) exp((-1/2)*x^2)
  proposal_sd_values = 2^seq(-10, 10, 1)
  n = length(proposal_sd_values)
  res = numeric(n)
  for (i in (1:n)) {
      res[i] = estimate_asymptotic_variance(gamma, proposal_sd_values[i], 100, 1000)
  }
  df = data.frame(estimated_mcmc_variance= res, proposal_sd = proposal_sd_values)

  # plot
  library(ggplot2)
  ggplot(df, aes(x = proposal_sd, y = estimated_mcmc_variance)) +
    geom_line() +
    geom_point() +
    scale_x_continuous(trans = "log2", breaks = proposal_sd_values) +
    labs(x = "Proposal SD (log2 scale)",
        y = "Estimated asymptotic variance",
        title = "Estimated asymptotic variance vs proposal SD") + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
\end{lstlisting}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth, height=0.4\textheight]{est_var.png}
\caption{Plot of estimated variances of different proposal sd's}
\label{fig:est_var}
\end{figure}
From Figure \ref{fig:est_var}, it can be observed that if the proposal sd are in the range of $\{1, 2, 4\}$, then the estimated asymptotic variance seems to be optimal.

For $\gamma(x) = \exp(-\frac1{2000}x^2)$, since the standard deviation of the target increases, then the target is more spreaded. Using the same optimal sd from above would cause the samples more concentrated at the initial point compared to the true target. So this will give a wrong estimation of the variance of target. While on the other side, for $\gamma(x) = \exp(-\frac{1}{2000}(x - 100)^2)$, because the target has the same variance as above but it is merely shifted, so in this case, the optimal proposal sd will stay roughly the same.
 
\end{document}

