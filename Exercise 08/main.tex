% Template for solutions write-ups, STAT 460/560
% Some basic notation is defined in 'macros/basic-math-macros'

\documentclass{article}
\input{macros/solutions-template}  % DO NOT CHANGE
\input{macros/typesetting-macros}  % DO NOT CHANGE
\input{macros/basic-math-macros} 
\graphicspath{{./figures/}}







\begin{document}



% FILL IN:
%  - YOUR NAME, YOUR EMAIL (self-explanatory)
%  - The assignment number goes in ##
\problemset{Junsong Tang}{junsong.tang@stat.ubc.ca}{Exercise 8}



% WRITE YOUR SOLUTION TO THE FIRST QUESTION
\qsol{Calibration analysis via cross-validation}
\begin{enumerate}
\item 
Below is the Stan code for the ``hubble\_pred'' model:
\begin{lstlisting}[language=R]
  data {
    int<lower=0> N; // number of observations
    vector[N] xs;   // independent variable
    vector[N] ys;   // dependent variable
    real x_pred;  //independent variable for the left-out point
  }

  parameters {
    real slope;
    real<lower=0> sigma;
  }

  model {
    // prior
    slope ~ student_t(3, 0, 100);
    sigma ~ exponential(0.001);

    // likelihood
    ys ~ normal(slope*xs, sigma);
  }

  generated quantities {
    real y_pred = normal_rng(slope * x_pred, sigma); 
  }
\end{lstlisting}


\item 
\begin{lstlisting}[language=R]
  N_obs = nrow(df)
  N_train = N_obs-1
  train_test_dta = list(
      N  = N_train,
      xs = df$distance[-N_obs], 
      ys = df$velocity[-N_obs], 
      x_pred = df$distance[N_obs]
  )
  fit = sampling(stan_model("hubble_predict.stan"), 
  data=list(N=train_test_dta$N, ys=train_test_dta$ys, xs=train_test_dta$xs, x_pred=train_test_dta$x_pred), 
  iter=5000, control = list(max_treedepth = 15))

  quantile(extract(fit)$y_pred, prob=c(0.1, 0.9))
  # 0.485201900838154, 1.13324592497241
\end{lstlisting}
It can be seen that the leave-one-out $80\%$ credible interval for ``y\_pred'' quantity is: $[0.48, 1.14]$.

\item 
\begin{lstlisting}[language=R]
  # 3
  ci_limits = matrix(, nrow = N_obs, ncol = 2)
  for (i in (1:N_obs)) {
      trained = list(N=N_train, xs=df$distance[-i], ys=df$velocity[-i], x_pred=df$distance[i])
      fit_i = sampling(stan_model("hubble_predict.stan"), 
      data=list(N=trained$N, ys=trained$ys, xs=trained$xs, x_pred=trained$x_pred), 
      iter=5000, control = list(max_treedepth = 15))
      ci_limits[i,] = as.vector(quantile(extract(fit_i)$y_pred, prob=c(0.1, 0.9)))
  }
  merged_df = df %>% 
  bind_cols(data.frame(CI_L = ci_limits[,1], CI_R = ci_limits[,2])) %>% 
  mutate(Inside_CI = (velocity >= CI_L & velocity <= CI_R)) 
  merged_df %>% 
    ggplot(aes(x = 1:N_obs, y = velocity, ymin = CI_L, ymax = CI_R, color=Inside_CI)) +
    geom_point() + 
    geom_errorbar() +
    theme_minimal() +
    labs(x = "Point", y = "Velocity")
\end{lstlisting}
The plot of $80\%$ credible intervals for all observations are given as in Figure \ref{fig:ci}, hence it can be observed that the number of intervals that fail to capture the true points is $4$, so the proportion of such successful intervals is $\frac56 \approx 0.83$, which is similar to the nominal coverage level.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth, height=0.4\textheight]{ci.png}
  \caption{Plot of $80\%$ credible intervals for each observation}
  \label{fig:ci}
\end{figure}

\end{enumerate}



\qsol{Estimating asymptotic variance}
\begin{lstlisting}[language=R]
  # function
  estimate_asymptotic_variance = function(gamma, proposal_sd, C, S) {
    sum_E = 0
    sum_Ec_sq = 0
    init_so_far = numeric(C)
    counter = 0
    for (c in (1:C)) {    
        init_pt = rnorm(1, 0, 1)
        while (init_pt %in% init_so_far) {
            init_pt = rnorm(1, 0, 1)
        }
        counter = counter + 1
        init_so_far[counter] = init_pt
        E_c = mean(simple_mh(gamma, init_pt, S, proposal_sd))
        sum_E = sum_E + E_c * S
        sum_Ec_sq = sum_Ec_sq + E_c^2
    }
    return ((S/C)*sum_Ec_sq - S*(sum_E/(C*S))^2)
  }
\end{lstlisting}


\begin{lstlisting}[language=R]
  # generate the data frame for variances
  gamma = function(x) exp((-1/2)*x^2)
  proposal_sd_values = 2^seq(-10, 10, 1)
  n = length(proposal_sd_values)
  res = numeric(n)
  for (i in (1:n)) {
      res[i] = estimate_asymptotic_variance(gamma, proposal_sd_values[i], 100, 1000)
  }
  df = data.frame(estimated_mcmc_variance= res, proposal_sd = proposal_sd_values)

  # plot
  library(ggplot2)
  ggplot(df, aes(x = proposal_sd, y = estimated_mcmc_variance)) +
    geom_line() +
    geom_point() +
    scale_x_continuous(trans = "log2", breaks = proposal_sd_values) +
    labs(x = "Proposal SD (log2 scale)",
        y = "Estimated asymptotic variance",
        title = "Estimated asymptotic variance vs proposal SD") + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
\end{lstlisting}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth, height=0.4\textheight]{est_var.png}
\caption{Plot of estimated variances of different proposal sd's}
\label{fig:est_var}
\end{figure}
From Figure \ref{fig:est_var}, it can be observed that if the proposal sd are in the range of $\{1, 2, 4\}$, then the estimated asymptotic variance seems to be optimal.

For $\gamma(x) = \exp(-\frac1{2000}x^2)$, since the standard deviation of the target increases, then the target is more spreaded. Using the optimal sd from above would cause the samples more concentrated at the initial point compared to the true target. So this will give a wrong estimation of the variance of target. While on the other side, for $\gamma(x) = \exp(-\frac{1}{2000}(x - 100)^2)$, because the target has the same variance as above but is merely shifted, so in this case, the optimal proposal sd will stay the same.
 
\end{document}

