% Template for solutions write-ups, STAT 460/560
% Some basic notation is defined in 'macros/basic-math-macros'

\documentclass{article}
\input{macros/solutions-template}  % DO NOT CHANGE
\input{macros/typesetting-macros}  % DO NOT CHANGE
\input{macros/basic-math-macros} 
\graphicspath{{./figures/}}







\begin{document}



% FILL IN:
%  - YOUR NAME, YOUR EMAIL (self-explanatory)
%  - The assignment number goes in ##
\problemset{Junsong Tang}{junsong.tang@stat.ubc.ca}{Exercise 1}



% WRITE YOUR SOLUTION TO THE FIRST QUESTION
\qsol{sampling from a joint distribution} % USE THE SAME TITLES AS ON THE ASSIGNMENT SHEET
\begin{enumerate}
  
  \item 
  Since flips are indenpendent, then the joint pmf: $p(x, y_1, y_2, y_3, y_4) = p(y_1, y_2, y_3, y_4 |x) \cdot p(x) = \prod_{i=1}^4 p(y_i | x) \cdot p(x)$.

  Let $g(X, Y_1, Y_2, Y_3, Y_4) = (1 + Y_1)^X$, then by LOTUS, 
  \begin{align*}
    & \E (1 + Y_1)^X = \E g(X, Y_1, Y_2, Y_3, Y_4) \\
    & = \sum_{x \in \{0,1,2\}} \sum_{y_1\in \{0,1\}} \sum_{y_2} \sum_{y_3} \sum _{y_4} g(x, y_1, y_2, y_3, y_4) \cdot p(x, y_1, y_2, y_3, y_4)\\
    & = \sum_{x}\sum_{y_1} (1+y_1)^x \cdot p(y_1|x)\cdot p(x) \sum_{y_2} p(y_2|x) \sum_{y_3} p(y_3|x) \sum_{y_4} p(y_4|x)\\
    & = \sum_{x}\sum_{y_1} (1+y_1)^x \cdot p(y_1|x)\cdot p(x)\\
    & = \frac{13}{6}
  \end{align*}
  % Since $\forall j \in \N, Y_j | X \sim \Bernoulli(\frac{X}{2})$, then $\E(Y_j | X = i) = \frac{i}{2}$ and $\E (Y_j^2 | X = i) = \frac{i}{2}$, hence:
  % \begin{align*}
  %   & \E [(1 + Y_1)^X] = \E_X \E_{Y|X}[(1+Y_1)^X | X)]\\
  %   & = \sum_{i = 0}^2 \E ((1+Y_1)^X | X=i) \cdot \P(X = i)\\
  %   & = 1\cdot \frac13 + \E (1 + Y_1 | X = 1) \cdot \frac13 + \E [(1+Y_1)^2 |X = 2] \cdot \frac13\\
  %   & = \frac13 + \frac13 \cdot (1 + \frac12) + (1 + 2 + 1) \cdot \frac13 = \frac{13}{6}\\
  % \end{align*}

  \item 
  \begin{lstlisting}[language=R]
    library(extraDistr)
    set.seed(2024)
    # theoretical expectation
    X = c(0,1,2)
    Y = c(0,1)
    p_mtx = matrix(c(1,1/2,0,0,1/2,1), byrow = FALSE, nrow=3)
    sum = 0
    for (i in (1:length(X))) {
      x = X[i]
      for (j in (1:length(Y))) {
        y = Y[j]
        p = p_mtx[i,j]
        sum = sum + (1+y)^x * p *(1/3)
      }
    }
    sum #2.16666666666667

    # sample function
    forward_sample = function() {
      x = rdunif(1, 0, 2)
      Y = rbinom(4, 1, x/2)
      return (c(x, Y))
    }
    forward_sample()
    # 1 1 0 1 0

    # function of (x, Y)
    f_eval = function(v) ((1 + v[2])^v[1])

    # compute expectation
    mean(replicate(f_eval(forward_sample()), n = 100000))
    # 2.16713
  \end{lstlisting}

  \item we use the sample mean of the function values of $g = (1+y_1)^x$ to approximate the expectation by the LLN
  
  \item From the simulation results, if the number of simulation gets larger, then the sample mean is closer to the true expectation.

  
\end{enumerate}







% WRITE YOUR SOLUTION TO THE SECOND QUESTION
\qsol{computing a conditional}
\begin{enumerate}
\item If $p = \frac12$, then $X = 1$, so we want: $\P(X = 1 | (Y_1, Y_2, Y_3, Y_4) = (0,0,0,0))$
\item Note that 
\[\P(Y_i = 0, \forall i | X = 0) = 1\] and \[\P(Y_i = 0, \forall i | X = 1) = (\frac12)^4\] and \[\P(Y_i = 0, \forall i | X = 2) = 0\] , so by Bayes rule and total probability:
\begin{align*}
& \P(X = 1 | (Y_1, Y_2, Y_3, Y_4) = (0,0,0,0)) \\
& = \frac{\P(Y_i = 0, \forall i | X = 1) \cdot \P(X = 1)}{\sum_{j = 0}^2 \P(Y_i = 0, \forall i | X = j) \cdot \P(X = j)}\\
& = \frac{(\frac12)^4  \cdot \frac13}{1\cdot \frac13 + (\frac12)^4 \cdot \frac13 + 0 \cdot \frac13}\\
& = \frac{1}{17}
\end{align*}
\end{enumerate}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\qsol{non uniform prior on coin types}
\begin{enumerate}
  \item \begin{align*}
  & X \sim Categorical((0,1,2) | (\frac{1}{100}, \frac{98}{100}, \frac{1}{100}))\\
  & Y_i | X \sim \Bernoulli(\frac{X}{2})
  \end{align*}

  \item \begin{align*}
    & \P(X = 1 | (Y_1, Y_2, Y_3, Y_4) = (0,0,0,0)) \\
    & = \frac{\P(Y_i = 0, \forall i | X = 1) \cdot \P(X = 1)}{\sum_{j = 0}^2 \P(Y_i = 0, \forall i | X = j) \cdot \P(X = j)}\\
    & = \frac{(\frac12)^4  \cdot \frac{98}{100}}{1\cdot \frac{1}{100} + (\frac12)^4 \cdot \frac{98}{100} + 0 \cdot \frac{1}{100}}\\
    & = \frac{98}{114}
  \end{align*}
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\qsol{a first posterior inference algorithm}
\begin{enumerate}
  \item 
  \begin{lstlisting}[language=R]
  posterior_given_four_heads = function(rho) {
    K = length(rho)
    posterior = NULL
    sum = 0
    for (k in (1 : K)) {
      p_k = (1-(k-1)/(K-1))^4 * rho[k] #joint p(Y, X = k)
      sum = sum + p_k
      posterior = c(posterior, p_k)
    }
    # normalizing
    posterior = posterior / sum
    return (posterior)
  }
  
  
  \end{lstlisting}

  \item \begin{lstlisting}[language=R]
    posterior_given_four_heads(c(1/100,98/100,1/100))[2] - 98/114
  # 0
    \end{lstlisting}
  
  
  \item 
  \begin{lstlisting}[language=R]
    rho = seq(1,10,1)
    rho = rho / sum(rho)
    posterior_given_four_heads(rho)
    # 0.201845869866174, 0.252022765728349, 0.221596677434241, 0.159483156437471, 0.0961390555299185, 0.0472542685740655, 0.0174434702353484, 0.00393785571450546, 0.000276880479926166, 0
    \end{lstlisting}
    From the posterior distribution, we can infer that when we have four heads: $(0,0,0,0)$, then it is most likely that $X = 2$. i.e. type $2$ coin.
\end{enumerate}



\qsol{}
\begin{enumerate}
  \item 
  \[X \sim \]


  \item  
  \begin{lstlisting}[language=R]
  posterior = function(rho, n_heads, n_observations) {
  posterior = NULL
  K = length(rho) - 1
  # normalize rho
  rho = rho / sum(rho)
  sum = 0
  for (k in (0:K)) {
    p_k = dbinom(n_heads, n_observations, 1-k/K) * rho[k+1]
    sum = sum + p_k
    posterior = c(posterior, p_k)
  }
  # normalizing
  posterior = posterior / sum
  return (posterior)
}
  \end{lstlisting}

  \item 
  \begin{lstlisting}[language=R]
    rho = seq(from=1, to=10, by=1)
    n_heads = 2
    n_observations = 10
    posterior(rho, n_heads, n_observations)  
  \end{lstlisting}
  
\end{enumerate}





% Optional: Feedback on assignment
% \qsol{Feedback on assignment}

 
\end{document}

